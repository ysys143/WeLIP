# WeLIP (Wedatalab LLM Infrastructure Platform) 상세 설계 및 실행 계획

## 1. 구축 목적 및 기본 방향

WeLIP은 대규모 언어모델(LLM)을 기반으로 하는 다양한 애플리케이션(RAG, Agent, 평가 등)을 통합적으로 지원하는 인프라 플랫폼으로, 현대적인 AI 운영 환경의 핵심 요구사항을 충족하도록 설계되었습니다. 본 플랫폼은 평가와 응용 중심의 경량화된 초기 환경을 구성하면서도, 향후 파인튜닝 및 정렬까지 확장 가능하게 설계되어 있습니다.

기본 방향은 온프레미스 환경을 전제로 하되, GCP에서 빠르게 구축하고 이후 내릴 수 있도록 클라우드-온프레미스 이식성을 확보하는 것입니다. 모든 구성요소는 컨테이너화되며, Kubernetes를 통해 오케스트레이션되거나 K8s-free 환경에서도 최소 구동 가능하도록 설정됩니다. 이를 통해 조직의 다양한 인프라 환경에서 일관된 운영이 가능하며, 초기 투자 비용을 최소화하면서도 점진적 확장이 가능한 구조를 제공합니다.

MVP(Minimum Viable Product) 접근법을 통해 초기에는 추론 기반 RAG/Agent 흐름을 먼저 구성하고, 안정적인 운영이 확보된 후 병렬화, 파인튜닝, 자동화된 평가 시스템 등으로 점진적 확장을 진행합니다. 이러한 단계적 접근을 통해 조기에 실제 업무에 활용 가능한 시스템을 구축하면서도, 장기적으로는 완전한 LLM 운영 플랫폼으로 진화할 수 있는 확장성을 보장합니다.

## 2. 구현 목표

WeLIP의 핵심 구현 목표는 LLM 기반 모델을 서빙하고, 이를 API 및 Agent로 호출하는 추론 파이프라인을 구현하는 것입니다. 이를 위해 vLLM을 기반으로 한 고성능 추론 서버를 구축하고, LangChain과 LangGraph를 활용한 복잡한 Agent 워크플로우를 지원합니다. 추론 파이프라인은 단순한 질의응답부터 복잡한 멀티스텝 추론까지 다양한 사용 사례를 지원하며, 실시간 응답과 배치 처리 모두를 효율적으로 처리할 수 있도록 설계됩니다.

모델 성능 평가와 실험 추적, 벤치마크 시스템 구성을 통해 지속적인 모델 개선과 성능 모니터링을 지원합니다. Weights & Biases를 중심으로 한 실험 추적 시스템은 모델 성능, 하이퍼파라미터, 데이터셋 버전 등을 체계적으로 관리하며, 재현 가능한 실험 환경을 제공합니다. 자체 개발한 평가 스크립트(harness)를 통해 rule-based 평가부터 GPT-as-a-judge 방식까지 다양한 평가 방법론을 지원합니다.

학습 데이터 수집, 전처리, 튜닝, 평가를 포함한 완전한 파인튜닝 파이프라인을 구성하여 조직의 특정 요구사항에 맞춘 모델 개발을 지원합니다. Ray를 활용한 분산 전처리 시스템과 Unsloth 기반 LoRA 튜닝, VeRL 기반 PPO/GRPO 학습을 통해 효율적인 모델 개발 환경을 제공합니다. 전체 시스템 상태 및 요청 흐름에 대한 모니터링 및 관찰성 확보를 통해 안정적인 운영과 신속한 문제 해결을 지원하며, 사용자 접근 제어 및 안전성 확보를 위한 인증·가드레일 체계를 도입하여 엔터프라이즈 환경에서 요구되는 보안 수준을 만족합니다.

## 3. 전체 아키텍처 개요

WeLIP의 전체 아키텍처는 마이크로서비스 기반의 분산 시스템으로 설계되며, 각 컴포넌트는 독립적으로 확장 가능하고 장애 격리가 가능한 구조를 갖습니다. 오케스트레이션 레이어는 Kubernetes를 기반으로 하며, Kubeflow Pipelines 또는 K8s-native Controller를 통해 복잡한 ML 워크플로우를 관리합니다. 이를 통해 데이터 전처리부터 모델 학습, 평가, 배포까지의 전체 라이프사이클을 자동화하고 관리할 수 있습니다.

분산 처리 및 실행 엔진으로는 Ray를 채택하여 대규모 데이터 처리와 모델 학습을 효율적으로 수행합니다. Ray는 Python 네이티브 분산 컴퓨팅 프레임워크로서 데이터 전처리, 하이퍼파라미터 튜닝, 분산 학습 등 다양한 ML 워크로드를 통합적으로 지원합니다. 모델 학습 및 정렬을 위해서는 PyTorch를 기반으로 하되, Unsloth를 통한 효율적인 LoRA 튜닝과 VeRL을 활용한 강화학습 기반 정렬을 지원합니다.

실험 추적 및 버전 관리는 Weights & Biases를 중심으로 구성되며, MinIO를 S3 호환 저장소로 활용하여 모델 아티팩트, 데이터셋, 실험 결과를 체계적으로 관리합니다. 모델 서빙은 vLLM을 주력으로 하되, 고가용성이 필요한 경우 Triton + KServe/ModelMesh 기반 이중화 구성을 지원합니다. 에이전트 구성은 LangChain과 LangGraph를 활용하여 복잡한 추론 체인과 워크플로우를 구현하며, 모니터링은 Prometheus/Grafana와 OpenTelemetry/Phoenix를 통해 시스템 메트릭과 애플리케이션 트레이스를 종합적으로 관리합니다. 인증 및 가드레일은 API Gateway를 통한 접근 제어와 Guardrails 라이브러리를 활용한 안전장치를 제공합니다.

## 4. 구현 순서 및 단계별 계획

### [0단계] 환경 준비 및 GCP 기반 베이스라인 구축

GCP Project 구성부터 시작하여 GKE(Google Kubernetes Engine) 클러스터를 구축합니다. 이 단계에서는 shared disk 설정, GPU 노드 구성, Helm 패키지 매니저 설치, Docker registry 설정 등 기본 인프라를 준비합니다. 특히 GPU 노드는 NVIDIA A100 또는 V100 기반으로 구성하며, 노드 풀 자동 확장 기능을 활용하여 비용 효율성을 확보합니다. MinIO를 S3 호환 저장소로 배포하고, Ray cluster를 구성하여 분산 처리 환경을 준비합니다. vLLM 등 필수 컴포넌트는 Helm 차트를 통해 배포하며, 모든 설정은 GitOps 방식으로 관리하여 재현 가능한 환경을 구축합니다.

### [1단계] 추론 및 에이전트 스택 구축

vLLM 기반 추론 서버를 실행하고 RESTful API 엔드포인트를 구성합니다. 이 단계에서는 Llama-2 7B 또는 Mistral 7B 수준의 모델을 로드하여 기본적인 텍스트 생성 API를 제공합니다. LangChain과 LangGraph를 활용하여 복잡한 Agent 워크플로우를 구현하고, 이를 FastAPI로 래핑하여 HTTP 서비스로 제공합니다. Phoenix와 OpenTelemetry를 통한 Agent trace 수집 설정을 구성하여 Agent 실행 과정의 상세한 추적이 가능하도록 합니다. Prompt 버전 관리 시스템을 구축하고, 샘플 프롬프트를 활용한 반복 실험 구조를 작성하여 프롬프트 엔지니어링 워크플로우를 지원합니다.

### [2단계] 실험 추적 및 벤치마크 시스템 구축

Weights & Biases 프로젝트를 구성하고, sweeps 기능을 활용한 하이퍼파라미터 튜닝, artifact 추적, Table 기능을 활용한 실험 결과 관리 구조를 설정합니다. 자체 평가 스크립트(harness)를 작성하여 rule-based 평가, GPT-as-a-judge 방식, 그리고 도메인 특화 평가 메트릭을 지원합니다. 평가 결과는 JSONL, Parquet, CSV 등 다양한 형식으로 저장할 수 있도록 표준화하고, 이를 자동화하는 스크립트를 개발합니다. 벤치마크 시스템은 일반적인 NLP 태스크부터 도메인 특화 태스크까지 다양한 평가를 지원하며, 결과는 대시보드를 통해 시각화됩니다.

### [3단계] 데이터 전처리 및 학습 파이프라인 구현

Ray Dataset과 task 기반 전처리 스크립트를 작성하여 대규모 데이터의 chunking, QA generation, 데이터 증강 등을 효율적으로 처리합니다. Instruction Tuning 포맷 변환(JSONL, Jinja 템플릿)과 Hard Negative 구성을 통해 고품질 학습 데이터를 생성합니다. Unsloth 기반 LoRA 튜닝 스크립트와 VeRL 기반 PPO/GRPO 학습 스크립트를 작성하여 효율적인 모델 파인튜닝을 지원합니다. 학습 파라미터와 결과는 W&B와 MinIO에 체계적으로 저장되며, 실험 재현성을 위한 버전 관리 시스템을 구축합니다. 분산 학습 환경에서의 체크포인트 관리와 장애 복구 메커니즘도 포함됩니다.

### [4단계] 서빙 및 버전 관리 체계 구축

MinIO 기반 모델 버전 관리 시스템을 구축하여 safetensors, GGUF 등 다양한 형식의 모델을 체계적으로 관리합니다. vLLM 또는 ModelMesh를 통한 모델 서빙부터 API Gateway를 거쳐 인증과 가드레일을 통과한 후 Agent가 호출하는 전체 흐름을 구성합니다. 모델, 데이터, 평가 결과의 버전을 Git, W&B, MinIO에서 3중으로 연동하여 완전한 추적성을 보장합니다. A/B 테스트를 위한 다중 모델 서빙과 트래픽 분산 기능을 구현하고, 모델 성능 모니터링을 통한 자동 롤백 메커니즘을 구축합니다.

### [5단계] 관찰성 및 운영 모니터링 구축

GPU 사용률, 노드 상태, Ray 클러스터 상태를 Prometheus로 수집하고 Grafana를 통해 시각화합니다. Agent 요청 흐름, LangChain 실행 트레이스, LLM 응답까지의 전체 과정을 Phoenix를 통해 추적하여 성능 병목점과 오류를 신속히 식별할 수 있도록 합니다. Latency, Token usage, Throughput 등 주요 성능 지표를 정량화하는 스크립트를 구성하고, 이를 기반으로 한 자동 알림 시스템을 구축합니다. 시스템 전체의 헬스체크와 장애 대응을 위한 런북(runbook)을 작성하여 운영 효율성을 높입니다.

## 5. 온프레미스 이식 계획

온프레미스 이식성은 WeLIP의 핵심 설계 원칙 중 하나로, 모든 구성요소가 클라우드 종속성 없이 동작하도록 설계됩니다. Kubernetes 환경에서 Helm 차트 또는 Docker Compose를 통해 배포되도록 구성하며, GCP 상에서 사용한 MinIO, Ray, vLLM 등의 모든 컴포넌트는 온프레미스 환경에서도 동일하게 동작하도록 resource specification과 docker-compose 설정을 포함합니다. 특히 GPU 리소스 관리와 네트워크 설정은 온프레미스 환경의 특성을 고려하여 유연하게 구성할 수 있도록 설계됩니다.

GCP 전용 기능인 Vertex AI, BigQuery 등은 사용하지 않으며, TensorFlow Extended(TFX) 대신 PyTorch + Ray + LangChain 기반으로 설계하여 온프레미스 호환성을 확보합니다. 스토리지는 MinIO를 통해 S3 호환 인터페이스를 제공하되, 온프레미스에서는 NFS, Ceph 등 다양한 백엔드 스토리지와 연동 가능하도록 구성합니다. 네트워크 설정은 인그레스 컨트롤러를 통해 외부 접근을 제어하며, 온프레미스 환경의 방화벽 정책과 통합될 수 있도록 설계됩니다.

데이터베이스와 메시지 큐는 PostgreSQL, Redis, RabbitMQ 등 오픈소스 솔루션을 활용하여 클라우드 의존성을 제거하고, 모든 설정은 Helm 차트와 Kubernetes 매니페스트로 관리하여 환경 간 일관성을 보장합니다. 이식 과정에서 발생할 수 있는 성능 차이나 설정 이슈를 최소화하기 위해 상세한 마이그레이션 가이드와 검증 스크립트를 제공합니다.

## 6. 초기 구축을 위한 가이드라인

초기 구축은 복잡성을 최소화하고 빠른 가치 실현을 목표로 합니다. 모델은 1B~3B 파라미터 수준으로 제한하여 학습 없이 추론 중심 구조를 먼저 구축하고, 안정적인 운영이 확보된 후 대규모 모델과 학습 기능을 점진적으로 추가합니다. 초기에는 Llama-2 7B, Mistral 7B, 또는 CodeLlama 7B 수준의 모델을 활용하여 기본적인 텍스트 생성과 코드 생성 기능을 제공합니다.

워크플로우는 Chunking → Retrieval → RAG → Agent → 평가 → 리포트 순으로 단일 흐름부터 테스트하며, 각 단계가 안정적으로 동작함을 확인한 후 다음 단계로 진행합니다. 이를 통해 시스템의 각 컴포넌트를 독립적으로 검증하고 디버깅할 수 있습니다. 초기 데이터셋은 공개 벤치마크 데이터(GLUE, SuperGLUE 등)를 활용하여 시스템 동작을 검증하고, 점진적으로 도메인 특화 데이터를 추가합니다.

Ray 병렬화, 학습 루프 통합, 운영 버전 tagging 등의 고급 기능은 기본 시스템이 안정화된 후 단계적으로 확장합니다. 모니터링과 로깅은 초기부터 구축하여 시스템 동작을 투명하게 관찰할 수 있도록 하며, 문제 발생 시 신속한 대응이 가능하도록 합니다. 초기 사용자는 개발팀으로 제한하여 피드백을 수집하고 시스템을 개선한 후, 점진적으로 사용자 범위를 확대합니다.

## 7. 정책 및 보안

WeLIP의 보안 체계는 다층 방어(Defense in Depth) 원칙을 따르며, API Gateway를 통한 중앙집중식 접근 제어를 구현합니다. 모든 API 호출은 API Gateway를 거쳐야 하며, 이를 통해 인증, 인가, 요청 로깅, 레이트 리미팅 등을 일관되게 적용합니다. 사용자 인증은 OAuth 2.0 / OpenID Connect 표준을 따르며, JWT 토큰을 활용한 상태 없는(stateless) 인증 방식을 채택합니다. 역할 기반 접근 제어(RBAC)를 통해 사용자별로 접근 가능한 리소스와 기능을 세밀하게 제어합니다.

Prompt와 응답에 대한 guardrail 기능은 LangChain Guardrails 또는 자체 개발한 validation 로직을 통해 구현됩니다. 이를 통해 유해한 콘텐츠 생성 방지, 개인정보 보호, 저작권 침해 방지 등의 안전장치를 제공합니다. 모든 사용자 요청과 시스템 응답은 감사 로그로 기록되며, 이는 보안 사고 대응과 컴플라이언스 준수를 위해 활용됩니다.

데이터 보안을 위해 전송 중 암호화(TLS 1.3)와 저장 시 암호화(AES-256)를 적용하며, 민감한 데이터는 별도의 암호화 키 관리 시스템을 통해 보호됩니다. 컨테이너 보안을 위해 이미지 스캐닝, 런타임 보안 모니터링, 네트워크 정책 등을 적용하고, 정기적인 보안 취약점 점검과 패치 관리를 수행합니다. 장애 복구와 비즈니스 연속성을 위한 백업 및 재해 복구 계획을 수립하고, 정기적인 복구 테스트를 실시합니다.

## 8. 문서화 및 유지보수

WeLIP의 모든 컴포넌트는 GitOps 구조로 관리되며, Infrastructure as Code(IaC) 원칙을 따릅니다. Helm chart, Kubernetes YAML 매니페스트, Dockerfile 등 모든 인프라 설정은 Git 저장소에서 버전 관리되며, 변경 사항은 Pull Request를 통해 검토하고 승인하는 과정을 거칩니다. 이를 통해 인프라 변경의 추적성과 재현성을 보장하고, 롤백이 필요한 경우 신속하게 대응할 수 있습니다.

버전별 실험 및 평가 기록은 Git, W&B, MinIO의 3중 연동을 통해 완전한 재현성(reproducibility)을 보장합니다. 코드 버전은 Git에서, 실험 메타데이터와 결과는 W&B에서, 모델과 데이터 아티팩트는 MinIO에서 관리하며, 이들 간의 연관성을 명확히 기록합니다. 각 실험은 고유한 ID를 가지며, 실험 설정, 사용된 데이터, 모델 파라미터, 평가 결과 등이 완전히 추적 가능하도록 설계됩니다.

기술 문서는 MkDocs를 기반으로 하며, Mermaid 다이어그램을 활용한 시각화를 포함하여 시스템 아키텍처와 워크플로우를 직관적으로 이해할 수 있도록 합니다. API 문서는 OpenAPI 3.0 표준을 따르며, 자동 생성된 문서와 예제 코드를 제공합니다. 운영 매뉴얼, 트러블슈팅 가이드, 성능 튜닝 가이드 등 실무진이 필요로 하는 문서를 체계적으로 관리하며, 정기적인 업데이트를 통해 최신 상태를 유지합니다.

지속적인 유지보수를 위해 자동화된 테스트 스위트를 구축하고, CI/CD 파이프라인을 통해 코드 품질을 관리합니다. 성능 모니터링과 용량 계획을 통해 시스템의 확장성을 사전에 관리하며, 정기적인 보안 점검과 업데이트를 통해 시스템의 안정성을 유지합니다. 사용자 피드백과 시스템 메트릭을 기반으로 한 지속적인 개선 프로세스를 운영하여, WeLIP이 조직의 요구사항 변화에 능동적으로 대응할 수 있도록 합니다.

---

이 문서에 따라 WeLIP 구축은 초기 최소 기능(MVP)을 구성한 후, 안정적 운영 및 기능 확장을 통해 정렬 및 자율 최적화가 가능한 LLM 운영 플랫폼으로 진화하는 것을 목표로 합니다. 