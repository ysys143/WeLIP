WeLIP (Wedatalab LLM Infrastructure Platform) 구축을 위한 전체 구현 목표와 단계별 구현 계획을 아래와 같이 상세히 정리한다. 이 문서는 온프레미스 및 클라우드(GCP) 환경에서 일관된 운영이 가능하도록 설계된 대규모 LLM·RAG·Agent 시스템의 설계 및 실행 지침을 제공한다.

1. 구축 목적 및 기본 방향

WeLIP은 대규모 언어모델(LLM)을 기반으로 하는 다양한 애플리케이션(RAG, Agent, 평가 등)을 통합적으로 지원하는 인프라 플랫폼이다. 목적은 다음과 같다:

* 평가와 응용 중심의 경량화된 초기 환경을 구성하면서도, 향후 파인튜닝 및 정렬까지 확장 가능하게 설계
* 온프레미스 환경을 전제로 하되, GCP에서 빠르게 구축하고, 이후 내릴 수 있도록 클라우드-온프레미스 이식성 확보
* 모든 구성요소는 컨테이너화되며, K8s를 통해 오케스트레이션되거나 K8s-free 환경에서도 최소 구동 가능하도록 설정

2. 구현 목표

* LLM 기반 모델을 서빙하고, 이를 API 및 Agent로 호출하는 추론 파이프라인 구현
* 모델 성능 평가와 실험 추적, 벤치마크 시스템 구성
* 학습 데이터 수집, 전처리, 튜닝, 평가를 포함한 파인튜닝 파이프라인 구성
* 전체 시스템 상태 및 요청 흐름에 대한 모니터링 및 관찰성 확보
* 사용자 접근 제어 및 안전성 확보를 위한 인증·가드레일 체계 도입

3. 전체 아키텍처 개요

* 오케스트레이션: Kubernetes + Kubeflow Pipelines or K8s-native Controller
* 분산 처리 및 실행: Ray
* 모델 학습 및 정렬: PyTorch + Unsloth + VeRL
* 실험 추적: Weights & Biases (W\&B)
* 저장소 및 버전 관리: MinIO (S3 compatible)
* 모델 서빙: vLLM (또는 Triton + KServe/ModelMesh 기반 이중화)
* 에이전트 구성: LangChain + LangGraph
* 평가 및 벤치마크: Python harness + W\&B Table
* 모니터링: Grafana/Prometheus + OpenTelemetry/Phoenix
* 인증 및 가드레일: API Gateway + Guardrails

4. 구현 순서 및 단계별 계획

\[0단계] 환경 준비 및 GCP 기반 베이스라인 구축

* GCP Project 구성 및 GKE(Google Kubernetes Engine) 클러스터 구축
* shared disk, GPU 노드, Helm, Docker registry 설정
* MinIO, Ray cluster, vLLM 등 필수 컴포넌트 Helm 기반 배포

\[1단계] 추론 및 에이전트 스택 구축

* vLLM 기반 추론 서버 실행 및 API 엔드포인트 구성
* LangChain + LangGraph를 통한 Agent 스크립트 작성 및 FastAPI로 래핑
* Phoenix + OTel을 통한 Agent trace 수집 설정
* Prompt 버전 관리 및 샘플 프롬프트 반복 실험 구조 작성

\[2단계] 실험 추적 및 벤치마크 시스템 구축

* W\&B 프로젝트 구성, sweeps, artifact 추적, Table 활용 구조 설정
* 자체 평가 스크립트(harness) 작성: rule-based, GPT-as-a-judge 등 포함
* 평가 결과 저장 포맷(jsonl, parquet, csv) 표준화 및 스크립트화

\[3단계] 데이터 전처리 및 학습 파이프라인 구현

* Ray Dataset 및 task 기반 전처리 스크립트 작성: chunk, qa generation 등
* IT 포맷 변환(jsonl, jinja 템플릿) 및 Hard Negative 구성
* Unsloth 기반 LoRA 튜닝 및 VeRL 기반 PPO/GRPO 학습 스크립트 작성
* 학습 파라미터 및 결과 W\&B + MinIO 저장 구조 정립

\[4단계] 서빙 및 버전 관리 체계 구축

* MinIO 기반 경로 버저닝(safetensors, gguf 등) 체계 설정
* vLLM/ModelMesh → API Gateway → 인증/가드레일 → Agent 호출 흐름 구성
* 모델/데이터/평가 버전 Git + W\&B + MinIO 3중 연동 정립

\[5단계] 관찰성 및 운영 모니터링 구축

* GPU, 노드, Ray 상태를 Prometheus → Grafana 시각화
* Agent 요청 흐름, LangChain 실행 트레이스, LLM 응답까지 Phoenix 추적
* Latency, Token usage, Throughput 로그 정량화 스크립트 구성

5. 온프레미스 이식 계획

* 모든 구성요소는 Kubernetes 환경에서 Helm 또는 Docker Compose로 배포되도록 설계
* GCP 상에서 사용한 MinIO, Ray, vLLM 등은 온프레미스에 동일하게 구성되도록 resource spec 및 docker-compose 설정 포함
* GCP only 기능(Vertex AI 등)은 미사용하고, TFX 대신 PyTorch + Ray + LangChain 기반으로 설계하여 온프레미스 호환성 확보

6. 초기 구축을 위한 가이드라인

* 모델은 1B\~3B급으로 제한하여 학습 없이 추론 중심 구조 먼저 구축
* Chunking → Retrieval → RAG → Agent → 평가 → 리포트 순으로 단일 흐름부터 테스트
* 이후 Ray 병렬화, 학습 루프 통합, 운영 버전 tagging 등 기능 확장

7. 정책 및 보안

* API Gateway 통해 호출 관리 및 access log 기록
* Prompt/응답 등에 대한 guardrail 기능(LangChain Guardrails or custom validation)
* 사용자 인증체계(OAuth 등)는 이후 외부 도입 필요

8. 문서화 및 유지보수

* 모든 컴포넌트는 GitOps 구조로 관리: Helm chart, YAML, Dockerfile 포함
* 버전별 실험 및 평가 기록은 Git + W\&B + MinIO 연동해 reproducibility 보장
* 문서화는 MkDocs + Mermaid 기반 시각화 포함한 tech doc 패키지로 관리

이 문서에 따라 WeLIP 구축은 초기 최소 기능(MVP)을 구성한 후, 안정적 운영 및 기능 확장을 통해 정렬 및 자율 최적화가 가능한 LLM 운영 플랫폼으로 진화하는 것을 목표로 한다.
